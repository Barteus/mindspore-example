{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "TBD - Description of the problem ..."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!python --version"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[31mERROR: Could not find a version that satisfies the requirement mindspore (from versions: none)\u001B[0m\r\n",
      "\u001B[31mERROR: No matching distribution found for mindspore\u001B[0m\r\n",
      "\u001B[33mWARNING: You are using pip version 21.3.1; however, version 22.1 is available.\r\n",
      "You should consider upgrading via the '/home/barteus/Work/tutorials/notebooks/venv2/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install mindspore tqdm requests wordcloud nltk pandas openpyxl"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Download and load data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import requests\n",
    "import tempfile\n",
    "from tqdm import tqdm\n",
    "from typing import IO\n",
    "from pathlib import Path\n",
    "\n",
    "# Set the storage path to `home_path/.mindspore_examples`.\n",
    "cache_dir = Path.home() / '.mindspore_examples'\n",
    "\n",
    "\n",
    "def http_get(url: str, temp_file: IO):\n",
    "    \"\"\"Download data using the requests library and visualize the process using the tqdm library.\"\"\"\n",
    "    req = requests.get(url, stream=True)\n",
    "    content_length = req.headers.get('Content-Length')\n",
    "    total = int(content_length) if content_length is not None else None\n",
    "    progress = tqdm(unit='B', total=total, unit_scale=True, unit_divisor=1024)\n",
    "    for chunk in req.iter_content(chunk_size=1024):\n",
    "        if chunk:\n",
    "            progress.update(len(chunk))\n",
    "            temp_file.write(chunk)\n",
    "    progress.close()\n",
    "\n",
    "\n",
    "def download(file_name: str, url: str):\n",
    "    \"\"\"Download data and save it with the specified name.\"\"\"\n",
    "    if not os.path.exists(cache_dir):\n",
    "        os.makedirs(cache_dir)\n",
    "    cache_path = os.path.join(cache_dir, file_name)\n",
    "    cache_exist = os.path.exists(cache_path)\n",
    "    if not cache_exist:\n",
    "        with tempfile.NamedTemporaryFile() as temp_file:\n",
    "            http_get(url, temp_file)\n",
    "            temp_file.flush()\n",
    "            temp_file.seek(0)\n",
    "            with open(cache_path, 'wb') as cache_file:\n",
    "                shutil.copyfileobj(temp_file, cache_file)\n",
    "    return cache_path"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "'/home/barteus/.mindspore_examples/aclImdb_v1.tar.gz'"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_path = download('aclImdb_v1.tar.gz',\n",
    "                     'https://mindspore-website.obs.myhuaweicloud.com/notebook/datasets/aclImdb_v1.tar.gz')\n",
    "imdb_path"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import re\n",
    "import six\n",
    "import string\n",
    "import tarfile\n",
    "\n",
    "\n",
    "class IMDBData():\n",
    "    \"\"\"IMDB dataset loader.\n",
    "\n",
    "    Load the IMDB dataset and processes it as a Python iteration object.\n",
    "\n",
    "    \"\"\"\n",
    "    label_map = {\n",
    "        \"pos\": 1,\n",
    "        \"neg\": 0\n",
    "    }\n",
    "\n",
    "    def __init__(self, path, mode=\"train\"):\n",
    "        self.mode = mode\n",
    "        self.path = path\n",
    "        self.docs, self.labels = [], []\n",
    "        self.stats = {\n",
    "            \"pos\": 0,\n",
    "            \"neg\": 0\n",
    "        }\n",
    "\n",
    "        self._load(\"pos\")\n",
    "        self._load(\"neg\")\n",
    "\n",
    "    def _load(self, label):\n",
    "        pattern = re.compile(r\"aclImdb/{}/{}/.*\\.txt$\".format(self.mode, label))\n",
    "        # Load data to the memory.\n",
    "        with tarfile.open(self.path) as tarf:\n",
    "            tf = tarf.next()\n",
    "            while tf is not None:\n",
    "                if bool(pattern.match(tf.name)):\n",
    "                    # Segment text, remove punctuations and special characters, and convert text to lowercase.\n",
    "                    self.docs.append(str(tarf.extractfile(tf).read().rstrip(six.b(\"\\n\\r\"))\n",
    "                                         .translate(None, six.b(string.punctuation)).lower()).split())\n",
    "                    self.labels.append([self.label_map[label]])\n",
    "                    self.stats[label] = self.stats[label] + 1\n",
    "                tf = tarf.next()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.docs[idx], self.labels[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.docs)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "import mindspore.dataset as dataset\n",
    "\n",
    "\n",
    "def load_imdb(imdb_path):\n",
    "    imdb_train = dataset.GeneratorDataset(IMDBData(imdb_path, \"train\"), column_names=[\"text\", \"label\"])\n",
    "    imdb_test = dataset.GeneratorDataset(IMDBData(imdb_path, \"test\"), column_names=[\"text\", \"label\"])\n",
    "    return imdb_train, imdb_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "data": {
      "text/plain": "<mindspore.dataset.engine.datasets_user_defined.GeneratorDataset at 0x7f75c6ac5fd0>"
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_train, imdb_test = load_imdb(imdb_path)\n",
    "imdb_train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_glove(glove_path):\n",
    "    glove_100d_path = os.path.join(cache_dir, 'glove.6B.100d.txt')\n",
    "    if not os.path.exists(glove_100d_path):\n",
    "        glove_zip = zipfile.ZipFile(glove_path)\n",
    "        glove_zip.extractall(cache_dir)\n",
    "\n",
    "    embeddings = []\n",
    "    tokens = []\n",
    "    with open(glove_100d_path, encoding='utf-8') as gf:\n",
    "        for glove in gf:\n",
    "            word, embedding = glove.split(maxsplit=1)\n",
    "            tokens.append(word)\n",
    "            embeddings.append(np.fromstring(embedding, dtype=np.float32, sep=' '))\n",
    "    # Add the embeddings corresponding to the special placeholders <unk> and <pad>.\n",
    "    embeddings.append(np.random.rand(100))\n",
    "    embeddings.append(np.zeros((100,), np.float32))\n",
    "\n",
    "    vocab = dataset.text.Vocab.from_list(tokens, special_tokens=[\"<unk>\", \"<pad>\"], special_first=False)\n",
    "    embeddings = np.array(embeddings).astype(np.float32)\n",
    "    return vocab, embeddings"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "data": {
      "text/plain": "400002"
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_path = download('glove.6B.zip', 'https://mindspore-website.obs.myhuaweicloud.com/notebook/datasets/glove.6B.zip')\n",
    "vocab, embeddings = load_glove(glove_path)\n",
    "len(vocab.vocab())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "data": {
      "text/plain": "(0,\n array([-0.038194, -0.24487 ,  0.72812 , -0.39961 ,  0.083172,  0.043953,\n        -0.39141 ,  0.3344  , -0.57545 ,  0.087459,  0.28787 , -0.06731 ,\n         0.30906 , -0.26384 , -0.13231 , -0.20757 ,  0.33395 , -0.33848 ,\n        -0.31743 , -0.48336 ,  0.1464  , -0.37304 ,  0.34577 ,  0.052041,\n         0.44946 , -0.46971 ,  0.02628 , -0.54155 , -0.15518 , -0.14107 ,\n        -0.039722,  0.28277 ,  0.14393 ,  0.23464 , -0.31021 ,  0.086173,\n         0.20397 ,  0.52624 ,  0.17164 , -0.082378, -0.71787 , -0.41531 ,\n         0.20335 , -0.12763 ,  0.41367 ,  0.55187 ,  0.57908 , -0.33477 ,\n        -0.36559 , -0.54857 , -0.062892,  0.26584 ,  0.30205 ,  0.99775 ,\n        -0.80481 , -3.0243  ,  0.01254 , -0.36942 ,  2.2167  ,  0.72201 ,\n        -0.24978 ,  0.92136 ,  0.034514,  0.46745 ,  1.1079  , -0.19358 ,\n        -0.074575,  0.23353 , -0.052062, -0.22044 ,  0.057162, -0.15806 ,\n        -0.30798 , -0.41625 ,  0.37972 ,  0.15006 , -0.53212 , -0.2055  ,\n        -1.2526  ,  0.071624,  0.70565 ,  0.49744 , -0.42063 ,  0.26148 ,\n        -1.538   , -0.30223 , -0.073438, -0.28312 ,  0.37104 , -0.25217 ,\n         0.016215, -0.017099, -0.38984 ,  0.87424 , -0.72569 , -0.51058 ,\n        -0.52028 , -0.1459  ,  0.8278  ,  0.27062 ], dtype=float32))"
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = vocab.tokens_to_ids('the')\n",
    "embedding = embeddings[idx]\n",
    "idx, embedding"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data preprocessing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "import mindspore\n",
    "\n",
    "lookup_op = dataset.text.Lookup(vocab, unknown_token='<unk>')\n",
    "pad_op = dataset.transforms.c_transforms.PadEnd([500], pad_value=vocab.tokens_to_ids('<pad>'))\n",
    "type_cast_op = dataset.transforms.c_transforms.TypeCast(mindspore.float32)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "imdb_train = imdb_train.map(operations=[lookup_op, pad_op], input_columns=['text'])\n",
    "imdb_train = imdb_train.map(operations=[type_cast_op], input_columns=['label'])\n",
    "\n",
    "imdb_test = imdb_test.map(operations=[lookup_op, pad_op], input_columns=['text'])\n",
    "imdb_test = imdb_test.map(operations=[type_cast_op], input_columns=['label'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(140205:140146708127680,MainProcess):2022-05-12-16:43:38.508.150 [mindspore/dataset/engine/datasets.py:1122] Dataset is shuffled before split.\n"
     ]
    }
   ],
   "source": [
    "imdb_train, imdb_valid = imdb_train.split([0.7, 0.3])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "imdb_train = imdb_train.batch(64, drop_remainder=True)\n",
    "imdb_valid = imdb_valid.batch(64, drop_remainder=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Modeling"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "import mindspore.nn as nn\n",
    "import mindspore.numpy as mnp\n",
    "import mindspore.ops as ops\n",
    "from mindspore import Tensor\n",
    "\n",
    "\n",
    "class RNN(nn.Cell):\n",
    "    def __init__(self, embeddings, hidden_dim, output_dim, n_layers,\n",
    "                 bidirectional, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "        vocab_size, embedding_dim = embeddings.shape\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, embedding_table=Tensor(embeddings),\n",
    "                                      padding_idx=pad_idx)\n",
    "        self.rnn = nn.LSTM(embedding_dim,\n",
    "                           hidden_dim,\n",
    "                           num_layers=n_layers,\n",
    "                           bidirectional=bidirectional,\n",
    "                           dropout=dropout,\n",
    "                           batch_first=True)\n",
    "        self.fc = nn.Dense(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(1 - dropout)\n",
    "        self.sigmoid = ops.Sigmoid()\n",
    "\n",
    "    def construct(self, inputs):\n",
    "        embedded = self.dropout(self.embedding(inputs))\n",
    "        _, (hidden, _) = self.rnn(embedded)\n",
    "        hidden = self.dropout(mnp.concatenate((hidden[-2, :, :], hidden[-1, :, :]), axis=1))\n",
    "        output = self.fc(hidden)\n",
    "        return self.sigmoid(output)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "output_size = 1\n",
    "num_layers = 2\n",
    "bidirectional = True\n",
    "dropout = 0.5\n",
    "lr = 0.001\n",
    "pad_idx = vocab.tokens_to_ids('<pad>')\n",
    "\n",
    "net = RNN(embeddings, hidden_size, output_size, num_layers, bidirectional, dropout, pad_idx)\n",
    "loss = nn.BCELoss(reduction='mean')\n",
    "net_with_loss = nn.WithLossCell(net, loss)\n",
    "optimizer = nn.Adam(net.trainable_params(), learning_rate=lr)\n",
    "train_one_step = nn.TrainOneStepCell(net_with_loss, optimizer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_dataset, epoch=0):\n",
    "    model.set_train()\n",
    "    total = train_dataset.get_dataset_size()\n",
    "    loss_total = 0\n",
    "    step_total = 0\n",
    "    with tqdm(total=total) as t:\n",
    "        t.set_description('Epoch %i' % epoch)\n",
    "        for i in train_dataset.create_tuple_iterator():\n",
    "            loss = model(*i)\n",
    "            loss_total += loss.asnumpy()\n",
    "            step_total += 1\n",
    "            t.set_postfix(loss=loss_total / step_total)\n",
    "            t.update(1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Calculate the accuracy of each batch.\n",
    "    \"\"\"\n",
    "\n",
    "    # Round off the predicted value.\n",
    "    rounded_preds = np.around(preds)\n",
    "    correct = (rounded_preds == y).astype(np.float32)\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "def evaluate(model, test_dataset, criterion, epoch=0):\n",
    "    total = test_dataset.get_dataset_size()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    step_total = 0\n",
    "    model.set_train(False)\n",
    "\n",
    "    with tqdm(total=total) as t:\n",
    "        t.set_description('Epoch %i' % epoch)\n",
    "        for i in test_dataset.create_tuple_iterator():\n",
    "            predictions = model(i[0])\n",
    "            loss = criterion(predictions, i[1])\n",
    "            epoch_loss += loss.asnumpy()\n",
    "\n",
    "            acc = binary_accuracy(predictions.asnumpy(), i[1].asnumpy())\n",
    "            epoch_acc += acc\n",
    "\n",
    "            step_total += 1\n",
    "            t.set_postfix(loss=epoch_loss / step_total, acc=epoch_acc / step_total)\n",
    "            t.update(1)\n",
    "\n",
    "    return epoch_loss / total"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "from mindspore import context\n",
    "\n",
    "context.set_context(mode=context.PYNATIVE_MODE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 273/273 [16:28<00:00,  3.62s/it, loss=0.678]\n",
      "Epoch 0: 100%|██████████| 117/117 [01:41<00:00,  1.15it/s, acc=0.603, loss=0.668]\n",
      "Epoch 1: 100%|██████████| 273/273 [14:27<00:00,  3.18s/it, loss=0.663]\n",
      "Epoch 1: 100%|██████████| 117/117 [01:43<00:00,  1.13it/s, acc=0.631, loss=0.677]\n",
      "Epoch 2: 100%|██████████| 273/273 [14:30<00:00,  3.19s/it, loss=0.63] \n",
      "Epoch 2: 100%|██████████| 117/117 [01:44<00:00,  1.12it/s, acc=0.717, loss=0.662]\n"
     ]
    }
   ],
   "source": [
    "from mindspore import save_checkpoint\n",
    "\n",
    "num_epochs = 3\n",
    "best_valid_loss = float('inf')\n",
    "ckpt_file_name = os.path.join(cache_dir, 'sentiment-analysis.ckpt')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_one_epoch(train_one_step, imdb_train, epoch)\n",
    "    valid_loss = evaluate(net, imdb_valid, loss, epoch)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        save_checkpoint(net, ckpt_file_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 391/391 [05:16<00:00,  1.23it/s, acc=0.5, loss=0.693]  \n"
     ]
    },
    {
     "data": {
      "text/plain": "0.6931147648550361"
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_test = imdb_test.batch(64)\n",
    "evaluate(net, imdb_test, loss)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Created based on: https://www.mindspore.cn/tutorials/application/en/r1.7/nlp/sentiment_analysis.html"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}