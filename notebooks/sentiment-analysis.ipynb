{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: python: command not found\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mindspore in /home/bpk/.local/lib/python3.8/site-packages (1.7.0)\n",
      "Requirement already satisfied: tqdm in /home/bpk/.local/lib/python3.8/site-packages (4.64.0)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (2.22.0)\n",
      "Requirement already satisfied: wordcloud in /home/bpk/.local/lib/python3.8/site-packages (1.8.1)\n",
      "Requirement already satisfied: nltk in /home/bpk/.local/lib/python3.8/site-packages (3.7)\n",
      "Requirement already satisfied: pandas in /home/bpk/.local/lib/python3.8/site-packages (1.4.2)\n",
      "Requirement already satisfied: openpyxl in /home/bpk/.local/lib/python3.8/site-packages (3.0.9)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/bpk/.local/lib/python3.8/site-packages (from mindspore) (9.1.0)\n",
      "Requirement already satisfied: asttokens>=2.0.0 in /home/bpk/.local/lib/python3.8/site-packages (from mindspore) (2.0.5)\n",
      "Requirement already satisfied: protobuf>=3.13.0 in /home/bpk/.local/lib/python3.8/site-packages (from mindspore) (3.20.1)\n",
      "Requirement already satisfied: scipy>=1.5.2 in /home/bpk/.local/lib/python3.8/site-packages (from mindspore) (1.8.0)\n",
      "Requirement already satisfied: psutil>=5.6.1 in /home/bpk/.local/lib/python3.8/site-packages (from mindspore) (5.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/bpk/.local/lib/python3.8/site-packages (from mindspore) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /home/bpk/.local/lib/python3.8/site-packages (from mindspore) (1.22.3)\n",
      "Requirement already satisfied: matplotlib in /home/bpk/.local/lib/python3.8/site-packages (from wordcloud) (3.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/bpk/.local/lib/python3.8/site-packages (from nltk) (2022.4.24)\n",
      "Requirement already satisfied: joblib in /home/bpk/.local/lib/python3.8/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk) (7.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/bpk/.local/lib/python3.8/site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/bpk/.local/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: et-xmlfile in /home/bpk/.local/lib/python3.8/site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from asttokens>=2.0.0->mindspore) (1.14.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/bpk/.local/lib/python3.8/site-packages (from packaging>=20.0->mindspore) (3.0.9)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/bpk/.local/lib/python3.8/site-packages (from matplotlib->wordcloud) (4.33.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/bpk/.local/lib/python3.8/site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/bpk/.local/lib/python3.8/site-packages (from matplotlib->wordcloud) (1.4.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install mindspore tqdm requests wordcloud nltk pandas openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Download and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import requests\n",
    "import tempfile\n",
    "from tqdm import tqdm\n",
    "from typing import IO\n",
    "from pathlib import Path\n",
    "\n",
    "# Set the storage path to `home_path/.mindspore_examples`.\n",
    "cache_dir = Path.home() / '.mindspore_examples'\n",
    "\n",
    "\n",
    "def http_get(url: str, temp_file: IO):\n",
    "    \"\"\"Download data using the requests library and visualize the process using the tqdm library.\"\"\"\n",
    "    req = requests.get(url, stream=True)\n",
    "    content_length = req.headers.get('Content-Length')\n",
    "    total = int(content_length) if content_length is not None else None\n",
    "    progress = tqdm(unit='B', total=total, unit_scale=True, unit_divisor=1024)\n",
    "    for chunk in req.iter_content(chunk_size=1024):\n",
    "        if chunk:\n",
    "            progress.update(len(chunk))\n",
    "            temp_file.write(chunk)\n",
    "    progress.close()\n",
    "\n",
    "\n",
    "def download(file_name: str, url: str):\n",
    "    \"\"\"Download data and save it with the specified name.\"\"\"\n",
    "    if not os.path.exists(cache_dir):\n",
    "        os.makedirs(cache_dir)\n",
    "    cache_path = os.path.join(cache_dir, file_name)\n",
    "    cache_exist = os.path.exists(cache_path)\n",
    "    if not cache_exist:\n",
    "        with tempfile.NamedTemporaryFile() as temp_file:\n",
    "            http_get(url, temp_file)\n",
    "            temp_file.flush()\n",
    "            temp_file.seek(0)\n",
    "            with open(cache_path, 'wb') as cache_file:\n",
    "                shutil.copyfileobj(temp_file, cache_file)\n",
    "    return cache_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/bpk/.mindspore_examples/aclImdb_v1.tar.gz'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_path = download('aclImdb_v1.tar.gz',\n",
    "                     'https://mindspore-website.obs.myhuaweicloud.com/notebook/datasets/aclImdb_v1.tar.gz')\n",
    "imdb_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import six\n",
    "import string\n",
    "import tarfile\n",
    "\n",
    "\n",
    "class IMDBData():\n",
    "    \"\"\"IMDB dataset loader.\n",
    "\n",
    "    Load the IMDB dataset and processes it as a Python iteration object.\n",
    "\n",
    "    \"\"\"\n",
    "    label_map = {\n",
    "        \"pos\": 1,\n",
    "        \"neg\": 0\n",
    "    }\n",
    "\n",
    "    def __init__(self, path, mode=\"train\"):\n",
    "        self.mode = mode\n",
    "        self.path = path\n",
    "        self.docs, self.labels = [], []\n",
    "        self.stats = {\n",
    "            \"pos\": 0,\n",
    "            \"neg\": 0\n",
    "        }\n",
    "\n",
    "        self._load(\"pos\")\n",
    "        self._load(\"neg\")\n",
    "\n",
    "    def _load(self, label):\n",
    "        pattern = re.compile(r\"aclImdb/{}/{}/.*\\.txt$\".format(self.mode, label))\n",
    "        # Load data to the memory.\n",
    "        with tarfile.open(self.path) as tarf:\n",
    "            tf = tarf.next()\n",
    "            while tf is not None:\n",
    "                if bool(pattern.match(tf.name)):\n",
    "                    # Segment text, remove punctuations and special characters, and convert text to lowercase.\n",
    "                    self.docs.append(str(tarf.extractfile(tf).read().rstrip(six.b(\"\\n\\r\"))\n",
    "                                         .translate(None, six.b(string.punctuation)).lower()).split())\n",
    "                    self.labels.append([self.label_map[label]])\n",
    "                    self.stats[label] = self.stats[label] + 1\n",
    "                tf = tarf.next()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.docs[idx], self.labels[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import mindspore.dataset as dataset\n",
    "\n",
    "\n",
    "def load_imdb(imdb_path):\n",
    "    imdb_train = dataset.GeneratorDataset(IMDBData(imdb_path, \"train\"), column_names=[\"text\", \"label\"])\n",
    "    imdb_test = dataset.GeneratorDataset(IMDBData(imdb_path, \"test\"), column_names=[\"text\", \"label\"])\n",
    "    return imdb_train, imdb_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mindspore.dataset.engine.datasets_user_defined.GeneratorDataset at 0x7f7e8157a310>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_train, imdb_test = load_imdb(imdb_path)\n",
    "imdb_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_glove(glove_path):\n",
    "    glove_100d_path = os.path.join(cache_dir, 'glove.6B.100d.txt')\n",
    "    if not os.path.exists(glove_100d_path):\n",
    "        glove_zip = zipfile.ZipFile(glove_path)\n",
    "        glove_zip.extractall(cache_dir)\n",
    "\n",
    "    embeddings = []\n",
    "    tokens = []\n",
    "    with open(glove_100d_path, encoding='utf-8') as gf:\n",
    "        for glove in gf:\n",
    "            word, embedding = glove.split(maxsplit=1)\n",
    "            tokens.append(word)\n",
    "            embeddings.append(np.fromstring(embedding, dtype=np.float32, sep=' '))\n",
    "    # Add the embeddings corresponding to the special placeholders <unk> and <pad>.\n",
    "    embeddings.append(np.random.rand(100))\n",
    "    embeddings.append(np.zeros((100,), np.float32))\n",
    "\n",
    "    vocab = dataset.text.Vocab.from_list(tokens, special_tokens=[\"<unk>\", \"<pad>\"], special_first=False)\n",
    "    embeddings = np.array(embeddings).astype(np.float32)\n",
    "    return vocab, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400002"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_path = download('glove.6B.zip', 'https://mindspore-website.obs.myhuaweicloud.com/notebook/datasets/glove.6B.zip')\n",
    "vocab, embeddings = load_glove(glove_path)\n",
    "len(vocab.vocab())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " array([-0.038194, -0.24487 ,  0.72812 , -0.39961 ,  0.083172,  0.043953,\n",
       "        -0.39141 ,  0.3344  , -0.57545 ,  0.087459,  0.28787 , -0.06731 ,\n",
       "         0.30906 , -0.26384 , -0.13231 , -0.20757 ,  0.33395 , -0.33848 ,\n",
       "        -0.31743 , -0.48336 ,  0.1464  , -0.37304 ,  0.34577 ,  0.052041,\n",
       "         0.44946 , -0.46971 ,  0.02628 , -0.54155 , -0.15518 , -0.14107 ,\n",
       "        -0.039722,  0.28277 ,  0.14393 ,  0.23464 , -0.31021 ,  0.086173,\n",
       "         0.20397 ,  0.52624 ,  0.17164 , -0.082378, -0.71787 , -0.41531 ,\n",
       "         0.20335 , -0.12763 ,  0.41367 ,  0.55187 ,  0.57908 , -0.33477 ,\n",
       "        -0.36559 , -0.54857 , -0.062892,  0.26584 ,  0.30205 ,  0.99775 ,\n",
       "        -0.80481 , -3.0243  ,  0.01254 , -0.36942 ,  2.2167  ,  0.72201 ,\n",
       "        -0.24978 ,  0.92136 ,  0.034514,  0.46745 ,  1.1079  , -0.19358 ,\n",
       "        -0.074575,  0.23353 , -0.052062, -0.22044 ,  0.057162, -0.15806 ,\n",
       "        -0.30798 , -0.41625 ,  0.37972 ,  0.15006 , -0.53212 , -0.2055  ,\n",
       "        -1.2526  ,  0.071624,  0.70565 ,  0.49744 , -0.42063 ,  0.26148 ,\n",
       "        -1.538   , -0.30223 , -0.073438, -0.28312 ,  0.37104 , -0.25217 ,\n",
       "         0.016215, -0.017099, -0.38984 ,  0.87424 , -0.72569 , -0.51058 ,\n",
       "        -0.52028 , -0.1459  ,  0.8278  ,  0.27062 ], dtype=float32))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = vocab.tokens_to_ids('the')\n",
    "embedding = embeddings[idx]\n",
    "idx, embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import mindspore\n",
    "\n",
    "lookup_op = dataset.text.Lookup(vocab, unknown_token='<unk>')\n",
    "pad_op = dataset.transforms.c_transforms.PadEnd([500], pad_value=vocab.tokens_to_ids('<pad>'))\n",
    "type_cast_op = dataset.transforms.c_transforms.TypeCast(mindspore.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "imdb_train = imdb_train.map(operations=[lookup_op, pad_op], input_columns=['text'])\n",
    "imdb_train = imdb_train.map(operations=[type_cast_op], input_columns=['label'])\n",
    "\n",
    "imdb_test = imdb_test.map(operations=[lookup_op, pad_op], input_columns=['text'])\n",
    "imdb_test = imdb_test.map(operations=[type_cast_op], input_columns=['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(222:139834086602560,MainProcess):2022-05-17-07:05:50.731.113 [mindspore/dataset/engine/datasets.py:1122] Dataset is shuffled before split.\n"
     ]
    }
   ],
   "source": [
    "imdb_train, imdb_valid = imdb_train.split([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "imdb_train = imdb_train.batch(64, drop_remainder=True)\n",
    "imdb_valid = imdb_valid.batch(64, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import mindspore.nn as nn\n",
    "import mindspore.numpy as mnp\n",
    "import mindspore.ops as ops\n",
    "from mindspore import Tensor\n",
    "\n",
    "\n",
    "class RNN(nn.Cell):\n",
    "    def __init__(self, embeddings, hidden_dim, output_dim, n_layers,\n",
    "                 bidirectional, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "        vocab_size, embedding_dim = embeddings.shape\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, embedding_table=Tensor(embeddings),\n",
    "                                      padding_idx=pad_idx)\n",
    "        self.rnn = nn.LSTM(embedding_dim,\n",
    "                           hidden_dim,\n",
    "                           num_layers=n_layers,\n",
    "                           bidirectional=bidirectional,\n",
    "                           dropout=dropout,\n",
    "                           batch_first=True)\n",
    "        self.fc = nn.Dense(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(1 - dropout)\n",
    "        self.sigmoid = ops.Sigmoid()\n",
    "\n",
    "    def construct(self, inputs):\n",
    "        embedded = self.dropout(self.embedding(inputs))\n",
    "        _, (hidden, _) = self.rnn(embedded)\n",
    "        hidden = self.dropout(mnp.concatenate((hidden[-2, :, :], hidden[-1, :, :]), axis=1))\n",
    "        output = self.fc(hidden)\n",
    "        return self.sigmoid(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "output_size = 1\n",
    "num_layers = 2\n",
    "bidirectional = True\n",
    "dropout = 0.5\n",
    "lr = 0.001\n",
    "pad_idx = vocab.tokens_to_ids('<pad>')\n",
    "\n",
    "net = RNN(embeddings, hidden_size, output_size, num_layers, bidirectional, dropout, pad_idx)\n",
    "loss = nn.BCELoss(reduction='mean')\n",
    "net_with_loss = nn.WithLossCell(net, loss)\n",
    "optimizer = nn.Adam(net.trainable_params(), learning_rate=lr)\n",
    "train_one_step = nn.TrainOneStepCell(net_with_loss, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_dataset, epoch=0):\n",
    "    model.set_train()\n",
    "    total = train_dataset.get_dataset_size()\n",
    "    loss_total = 0\n",
    "    step_total = 0\n",
    "    with tqdm(total=total) as t:\n",
    "        t.set_description('Epoch %i' % epoch)\n",
    "        for i in train_dataset.create_tuple_iterator():\n",
    "            loss = model(*i)\n",
    "            loss_total += loss.asnumpy()\n",
    "            step_total += 1\n",
    "            t.set_postfix(loss=loss_total / step_total)\n",
    "            t.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Calculate the accuracy of each batch.\n",
    "    \"\"\"\n",
    "\n",
    "    # Round off the predicted value.\n",
    "    rounded_preds = np.around(preds)\n",
    "    correct = (rounded_preds == y).astype(np.float32)\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, test_dataset, criterion, epoch=0):\n",
    "    total = test_dataset.get_dataset_size()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    step_total = 0\n",
    "    model.set_train(False)\n",
    "\n",
    "    with tqdm(total=total) as t:\n",
    "        t.set_description('Epoch %i' % epoch)\n",
    "        for i in test_dataset.create_tuple_iterator():\n",
    "            predictions = model(i[0])\n",
    "            loss = criterion(predictions, i[1])\n",
    "            epoch_loss += loss.asnumpy()\n",
    "\n",
    "            acc = binary_accuracy(predictions.asnumpy(), i[1].asnumpy())\n",
    "            epoch_acc += acc\n",
    "\n",
    "            step_total += 1\n",
    "            t.set_postfix(loss=epoch_loss / step_total, acc=epoch_acc / step_total)\n",
    "            t.update(1)\n",
    "\n",
    "    return epoch_loss / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from mindspore import context\n",
    "\n",
    "context.set_context(mode=context.PYNATIVE_MODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 273/273 [2:30:37<00:00, 33.10s/it, loss=0.696]  \n",
      "Epoch 0: 100%|██████████| 117/117 [02:59<00:00,  1.53s/it, acc=0.512, loss=0.692]\n",
      "Epoch 1: 100%|██████████| 273/273 [1:42:38<00:00, 22.56s/it, loss=0.695]  \n",
      "Epoch 1: 100%|██████████| 117/117 [02:45<00:00,  1.41s/it, acc=0.496, loss=0.697]\n",
      "Epoch 2: 100%|██████████| 273/273 [23:29<00:00,  5.16s/it, loss=0.69] \n",
      "Epoch 2: 100%|██████████| 117/117 [02:46<00:00,  1.43s/it, acc=0.544, loss=0.685]\n"
     ]
    }
   ],
   "source": [
    "from mindspore import save_checkpoint\n",
    "\n",
    "num_epochs = 3\n",
    "best_valid_loss = float('inf')\n",
    "ckpt_file_name = os.path.join(cache_dir, 'sentiment-analysis.ckpt')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_one_epoch(train_one_step, imdb_train, epoch)\n",
    "    valid_loss = evaluate(net, imdb_valid, loss, epoch)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        save_checkpoint(net, ckpt_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 391/391 [09:12<00:00,  1.41s/it, acc=0.501, loss=0.693]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6931539219053809"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_test = imdb_test.batch(64)\n",
    "evaluate(net, imdb_test, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Created based on: https://www.mindspore.cn/tutorials/application/en/r1.7/nlp/sentiment_analysis.html"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
